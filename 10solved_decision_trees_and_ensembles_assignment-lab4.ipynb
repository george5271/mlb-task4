{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s-RzEZEBR10z"
   },
   "source": [
    "# Основы машинного обучения: лабораторная работа №4\n",
    "## Деревья решений и ансамблевые методы\n",
    "\n",
    "В этой лабораторной работе вам предстоит реализовать с нуля алгоритм дерева решений, а затем применить и сравнить ансамблевые методы (бэггинг и бустинг).\n",
    "\n",
    "### Цель\n",
    "\n",
    "- Изучить теоретические основы и получить практические навыки реализации деревьев решений.\n",
    "- Освоить применение ансамблевых методов (бэггинг, бустинг) для решения задачи классификации.\n",
    "- Научиться сравнивать и анализировать производительность различных моделей машинного обучения.\n",
    "\n",
    "### Оценивание и баллы\n",
    "\n",
    "За это задание в общей сложности можно получить **до 10 баллов**. Баллы распределяются по задачам, как описано в ноутбуке."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YhdhwvrCw9c5"
   },
   "source": [
    "### 1. Определение варианта (0 баллов)\n",
    "\n",
    "Впишите свой номер в списке группы в ячейку ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l8k1q37Ww9c5"
   },
   "outputs": [],
   "source": [
    "# Впишите свой номер в списке группы\n",
    "STUDENT_ID = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JzKTd-a1w9c6",
    "outputId": "9d023b6b-ba13-43da-9104-5f50f72f0b78",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [],
   "source": [
    "# Не меняйте этот код\n",
    "datasets = [\n",
    "    ('Прогнозирование оттока клиентов банка', 'shubhammeshram579/bank-customer-churn-prediction'),\n",
    "    ('Прогнозирование инсульта', 'fedesoriano/stroke-prediction-dataset'),\n",
    "    ('Качество красного вина', 'uciml/red-wine-quality-cortez-et-al-2009'),\n",
    "    ('Прогнозирование сердечной недостаточности', 'fedesoriano/heart-failure-prediction'),\n",
    "    ('Набор данных о курении', 'kukuroo3/body-signal-of-smoking'),\n",
    "    ('Удержание клиентов телеком-оператора', 'blastchar/telco-customer-churn'),\n",
    "    ('Покупка в социальных сетях', 'rakeshpanigrahi/social-network-ads'),\n",
    "    ('Оценка риска по кредиту', 'uciml/german-credit'),\n",
    "    ('Прогнозирование диабета', 'uciml/pima-indians-diabetes-database'),\n",
    "    ('Обнаружение мошенничества с онлайн-платежами', 'rupakroy/online-payments-fraud-detection-dataset')\n",
    "]\n",
    "\n",
    "tree_algorithms = ['ID3', 'C4.5']\n",
    "bagging_algorithms = ['RandomForestClassifier', 'BaggingClassifier', 'ExtraTreesClassifier']\n",
    "boosting_algorithms = ['AdaBoostClassifier', 'GradientBoostingClassifier', 'XGBoost', 'CatBoost']\n",
    "\n",
    "variant = STUDENT_ID % 10 if STUDENT_ID else 10\n",
    "if variant == 0: variant = 10\n",
    "\n",
    "print(f\"Ваш вариант: {variant}\")\n",
    "print(f\"Датасет: {datasets[variant-1][0]}\")\n",
    "print(f\"URL для Kaggle API: {datasets[variant-1][1]}\")\n",
    "print(\"\\nЧасть 1. Алгоритм дерева для реализации:\")\n",
    "tree_algo_to_implement = tree_algorithms[(variant-1) % len(tree_algorithms)]\n",
    "print(f\"  - {tree_algo_to_implement}\")\n",
    "print(\"\\nЧасть 2. Алгоритмы для сравнения:\")\n",
    "bagging_algo_to_compare = bagging_algorithms[(variant-1) % len(bagging_algorithms)]\n",
    "boosting_algo_to_compare = boosting_algorithms[(variant-1) % len(boosting_algorithms)]\n",
    "print(f\"  - Бэггинг: {bagging_algo_to_compare}\")\n",
    "print(f\"  - Бустинг: {boosting_algo_to_compare}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6wtVjnqUw9c6"
   },
   "source": [
    "### 2. Загрузка и подготовка данных (1 балл)\n",
    "\n",
    "**2.1. Настройка Kaggle API**\n",
    "Чтобы скачать датасет напрямую с Kaggle, вам понадобится `kaggle.json` — ваш персональный токен. Его можно получить в личном кабинете на Kaggle (Your Profile -> Account -> API -> Create New API Token)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup_kaggle"
   },
   "outputs": [],
   "source": [
    "# Загрузите ваш kaggle.json файл\n",
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "if not os.path.exists('/root/.kaggle/kaggle.json'):\n",
    "    uploaded = files.upload()\n",
    "    for fn in uploaded.keys():\n",
    "        print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
    "            name=fn, length=len(uploaded[fn])))\n",
    "    # Создаем директорию и перемещаем файл\n",
    "    !mkdir -p ~/.kaggle\n",
    "    !mv kaggle.json ~/.kaggle/\n",
    "    !chmod 600 ~/.kaggle/kaggle.json\n",
    "    print(\"Kaggle API токен успешно настроен.\")\n",
    "else:\n",
    "    print(\"Kaggle API токен уже существует.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nrsth8kTw9c6"
   },
   "source": [
    "**2.2. Скачивание и загрузка датасета**\n",
    "Теперь можно скачать, разархивировать и загрузить данные в DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R_QmxpB5w9c6"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Шаг 1: Получаем URL датасета из определенного ранее варианта\n",
    "dataset_url = datasets[variant-1][1]\n",
    "\n",
    "# Шаг 2: Скачиваем датасет с помощью Kaggle API\n",
    "!kaggle datasets download -d {dataset_url}\n",
    "\n",
    "# Шаг 3: Определяем имя скачанного zip-архива\n",
    "zip_filename = dataset_url.split('/')[1] + '.zip'\n",
    "\n",
    "# Шаг 4: Разархивируем файл и АВТОМАТИЧЕСКИ находим имя CSV-файла\n",
    "csv_filename = None\n",
    "with zipfile.ZipFile(zip_filename, 'r') as z:\n",
    "    # Ищем первый файл с расширением .csv в архиве\n",
    "    for file_info in z.infolist():\n",
    "        if file_info.filename.endswith('.csv'):\n",
    "            csv_filename = file_info.filename\n",
    "            z.extract(csv_filename)\n",
    "            print(f\"Найден и разархивирован файл: {csv_filename}\")\n",
    "            break\n",
    "\n",
    "# Шаг 5: Загружаем данные в pandas DataFrame\n",
    "if csv_filename:\n",
    "    dataset = pd.read_csv(csv_filename)\n",
    "    print(f\"\\nДатасет {csv_filename} успешно загружен.\")\n",
    "else:\n",
    "    print(\"Ошибка: CSV-файл в архиве не найден.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l0lLa-xiR101"
   },
   "source": [
    "**2.3. Обзор и анализ данных**\n",
    "Проведите первичный анализ данных, чтобы понять их структуру."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 389
    },
    "id": "ujitZUYeR101",
    "outputId": "0981b2ba-7711-4f93-b6b6-8d6ef7179040"
   },
   "outputs": [],
   "source": [
    "print(\"Размер датасета:\")\n",
    "print(dataset.shape)\n",
    "\n",
    "print(\"\\nИнформация о типах данных и пропусках:\")\n",
    "dataset.info()\n",
    "\n",
    "print(\"\\nПервые 5 строк датасета:\")\n",
    "display(dataset.head())\n",
    "\n",
    "print(\"\\nСтатистическое описание числовых признаков:\")\n",
    "display(dataset.describe())\n",
    "\n",
    "print(\"\\nРаспределение целевой переменной 'isFraud':\")\n",
    "print(dataset['isFraud'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FGsJ_-hMXjSs"
   },
   "source": [
    "**Выводы по анализу данных:**\n",
    "\n",
    "Датасет содержит более 6.3 миллионов записей о финансовых транзакциях. Признаки включают шаг по времени, тип операции (CASH_IN, CASH_OUT, DEBIT, PAYMENT, TRANSFER), сумму, балансы до и после операции для отправителя и получателя. Целевая переменная — `isFraud`, бинарный признак (0 — не мошенничество, 1 — мошенничество). \n",
    "\n",
    "Ключевые наблюдения:\n",
    "- **Размер:** Датасет очень большой (6,362,620 записей, 11 признаков), что потребует эффективной обработки.\n",
    "- **Пропуски:** Пропущенных значений нет.\n",
    "- **Типы данных:** Большинство признаков числовые, но есть категориальный (`type`) и бинарный (`isFlaggedFraud`).\n",
    "- **Дисбаланс классов:** Классы сильно несбалансированы. Мошеннических транзакций (класс 1) всего 8213, что составляет около 0.13% от всех данных. Это необходимо будет учесть при оценке моделей.\n",
    "- **Задача:** Задача бинарной классификации — предсказать, является ли транзакция мошеннической на основе ее атрибутов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xfw6nVzoR104"
   },
   "source": [
    "**2.4. Предобработка данных**\n",
    "Подготовим данные для обучения моделей: выделим признаки и целевую переменную, разделим на обучающую и тестовую выборки, а также обработаем категориальные признаки и масштабируем числовые."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KI2IIJdHR104"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Удаляем неинформативные признаки (имена)\n",
    "dataset_processed = dataset.drop(['nameOrig', 'nameDest'], axis=1)\n",
    "\n",
    "# Определяем признаки (X) и целевую переменную (y)\n",
    "X = dataset_processed.drop('isFraud', axis=1)\n",
    "y = dataset_processed['isFraud']\n",
    "\n",
    "# Определяем категориальные и числовые признаки\n",
    "categorical_features = ['type']\n",
    "numerical_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Создаем конвейер для предобработки\n",
    "# - Категориальные признаки кодируем с помощью OneHotEncoder\n",
    "# - Числовые признаки масштабируем с помощью StandardScaler\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Разделяем данные на обучающую и тестовую выборки\n",
    "# Из-за дисбаланса классов используем стратификацию по 'y'\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Применяем предобработку: обучаем на X_train и трансформируем обе выборки\n",
    "X_train_prepared = preprocessor.fit_transform(X_train)\n",
    "X_test_prepared = preprocessor.transform(X_test)\n",
    "\n",
    "print(\"Данные успешно подготовлены и разделены.\")\n",
    "print(f\"Размер обучающей выборки (признаки): {X_train_prepared.shape}\")\n",
    "print(f\"Размер тестовой выборки (признаки): {X_test_prepared.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JRW5HPBzR106"
   },
   "source": [
    "### Часть 1: Реализация дерева решений (4 балла)\n",
    "\n",
    "В этой части вам предстоит реализовать с нуля алгоритм дерева решений, указанный в вашем варианте. **Нельзя использовать готовые реализации из библиотек.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XOhAxyK1R107"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "class Node:\n",
    "    \"\"\"Вспомогательный класс для хранения информации в узле дерева.\"\"\"\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, *, value=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "\n",
    "    def is_leaf_node(self):\n",
    "        return self.value is not None\n",
    "\n",
    "class DecisionTreeClassifier:\n",
    "    def __init__(self, min_samples_split=2, max_depth=100, n_features=None, criterion='id3'):\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.n_features = n_features\n",
    "        self.root = None\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.n_features = X.shape[1] if not self.n_features else min(X.shape[1], self.n_features)\n",
    "        self.root = self._grow_tree(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._traverse_tree(x, self.root) for x in X])\n",
    "\n",
    "    # --- Функции для реализации --- #\n",
    "\n",
    "    def _entropy(self, y):\n",
    "        \"\"\"Расчет энтропии.\n",
    "        H(S) = - sum_{c in C} p(c) * log2(p(c))\n",
    "        \"\"\"\n",
    "        ### BEGIN YOUR CODE ###\n",
    "        hist = np.bincount(y)\n",
    "        ps = hist / len(y)\n",
    "        return -np.sum([p * np.log2(p) for p in ps if p > 0])\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "    def _split_info(self, y_split):\n",
    "        \"\"\"Расчет информации о разделении (для C4.5).\n",
    "        SplitInfo(S, A) = - sum_{v in Values(A)} |Sv|/|S| * log2(|Sv|/|S|)\n",
    "        \"\"\"\n",
    "        ### BEGIN YOUR CODE ###\n",
    "        total_size = len(y_split)\n",
    "        if total_size == 0:\n",
    "            return 0\n",
    "        # Это по сути энтропия самого разделения\n",
    "        proportions = np.bincount(y_split) / total_size\n",
    "        return -np.sum([p * np.log2(p) for p in proportions if p > 0])\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "    def _information_gain(self, y, X_column, threshold):\n",
    "        parent_entropy = self._entropy(y)\n",
    "        left_idxs, right_idxs = self._split(X_column, threshold)\n",
    "        if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
    "            return 0\n",
    "        n = len(y)\n",
    "        n_l, n_r = len(left_idxs), len(right_idxs)\n",
    "        e_l, e_r = self._entropy(y[left_idxs]), self._entropy(y[right_idxs])\n",
    "        child_entropy = (n_l / n) * e_l + (n_r / n) * e_r\n",
    "        return parent_entropy - child_entropy\n",
    "\n",
    "    def _gain_ratio(self, y, X_column, threshold):\n",
    "        \"\"\"Расчет Gain Ratio (для C4.5).\n",
    "        GainRatio = InformationGain / SplitInfo\n",
    "        \"\"\"\n",
    "        ### BEGIN YOUR CODE ###\n",
    "        info_gain = self._information_gain(y, X_column, threshold)\n",
    "        if info_gain == 0:\n",
    "            return 0\n",
    "        \n",
    "        # Для SplitInfo нам нужно знать, как данные разделяются, а не их классы\n",
    "        left_idxs, right_idxs = self._split(X_column, threshold)\n",
    "        n = len(y)\n",
    "        n_l, n_r = len(left_idxs), len(right_idxs)\n",
    "        if n_l == 0 or n_r == 0:\n",
    "            return 0\n",
    "            \n",
    "        # SplitInfo это энтропия распределения по ветвям\n",
    "        split_entropy = -((n_l/n) * np.log2(n_l/n) + (n_r/n) * np.log2(n_r/n))\n",
    "\n",
    "        if split_entropy == 0:\n",
    "            return 0 # Избегаем деления на ноль\n",
    "            \n",
    "        return info_gain / split_entropy\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "    # --- Вспомогательные функции (менять не нужно) --- #\n",
    "\n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        n_samples, n_feats = X.shape\n",
    "        n_labels = len(np.unique(y))\n",
    "\n",
    "        # Критерии остановки\n",
    "        if (depth >= self.max_depth or n_labels == 1 or n_samples < self.min_samples_split):\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return Node(value=leaf_value)\n",
    "\n",
    "        feat_idxs = np.random.choice(n_feats, self.n_features, replace=False)\n",
    "        best_feature, best_thresh = self._best_criteria(X, y, feat_idxs)\n",
    "\n",
    "        if best_feature is None:\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return Node(value=leaf_value)\n",
    "\n",
    "        left_idxs, right_idxs = self._split(X[:, best_feature], best_thresh)\n",
    "        left = self._grow_tree(X[left_idxs, :], y[left_idxs], depth + 1)\n",
    "        right = self._grow_tree(X[right_idxs, :], y[right_idxs], depth + 1)\n",
    "        return Node(best_feature, best_thresh, left, right)\n",
    "\n",
    "    def _best_criteria(self, X, y, feat_idxs):\n",
    "        best_gain = -1\n",
    "        split_idx, split_threshold = None, None\n",
    "\n",
    "        for feat_idx in feat_idxs:\n",
    "            X_column = X[:, feat_idx]\n",
    "            thresholds = np.unique(X_column)\n",
    "            for thr in thresholds:\n",
    "                if self.criterion == 'id3':\n",
    "                    gain = self._information_gain(y, X_column, thr)\n",
    "                elif self.criterion == 'c4.5':\n",
    "                    # --- ВАШ КОД ЗДЕСЬ --- #\n",
    "                    # Используйте реализованную вами функцию _gain_ratio\n",
    "                    gain = self._gain_ratio(y, X_column, thr)\n",
    "                else:\n",
    "                    raise ValueError(\"Неизвестный критерий. Используйте 'id3' или 'c4.5'\")\n",
    "\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    split_idx = feat_idx\n",
    "                    split_threshold = thr\n",
    "\n",
    "        return split_idx, split_threshold\n",
    "\n",
    "    def _split(self, X_column, split_thresh):\n",
    "        left_idxs = np.argwhere(X_column <= split_thresh).flatten()\n",
    "        right_idxs = np.argwhere(X_column > split_thresh).flatten()\n",
    "        return left_idxs, right_idxs\n",
    "\n",
    "    def _traverse_tree(self, x, node):\n",
    "        if node.is_leaf_node():\n",
    "            return node.value\n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self._traverse_tree(x, node.left)\n",
    "        return self._traverse_tree(x, node.right)\n",
    "\n",
    "    def _most_common_label(self, y):\n",
    "        if len(y) == 0:\n",
    "            return None\n",
    "        counter = Counter(y)\n",
    "        return counter.most_common(1)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wDa2I38pkZVg"
   },
   "source": [
    "**Обучение и оценка вашего дерева**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LXMyhl80kLMZ"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# --- ОПТИМИЗАЦИЯ: Ускорение обучения на большом датасете ---\n",
    "# Ваш самописный алгоритм может работать очень долго на миллионах записей.\n",
    "# Чтобы проверить его работоспособность, обучим его на небольшой случайной выборке (семпле) данных.\n",
    "# Это стандартная практика при отладке и демонстрации алгоритмов.\n",
    "\n",
    "SAMPLE_SIZE = 10000  # Возьмем 10 000 случайных записей для обучения\n",
    "\n",
    "# Проверяем, достаточно ли данных для такого семпла\n",
    "if X_train_prepared.shape[0] > SAMPLE_SIZE:\n",
    "    # Генерируем случайные индексы для нашей выборки\n",
    "    random_indices = np.random.choice(X_train_prepared.shape[0], SAMPLE_SIZE, replace=False)\n",
    "    \n",
    "    # Создаем обучающую выборку меньшего размера\n",
    "    # Преобразуем разреженную матрицу в плотный массив для семплирования\n",
    "    X_train_sample = X_train_prepared[random_indices].toarray()\n",
    "    y_train_sample = y_train.to_numpy()[random_indices]\n",
    "    \n",
    "    print(f\"Обучение будет производиться на случайной выборке из {SAMPLE_SIZE} записей для ускорения.\")\n",
    "else:\n",
    "    # Если данных меньше, используем все что есть, преобразовав в плотный массив\n",
    "    X_train_sample = X_train_prepared.toarray()\n",
    "    y_train_sample = y_train.to_numpy()\n",
    "    print(\"Данных меньше, чем размер выборки, обучение будет на всем наборе.\")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "# Создаем и обучаем экземпляр классификатора на СЕМПЛЕ\n",
    "# Устанавливаем criterion='

