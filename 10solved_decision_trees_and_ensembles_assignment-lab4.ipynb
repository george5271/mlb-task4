{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s-RzEZEBR10z"
   },
   "source": [
    "# Основы машинного обучения: лабораторная работа №4\n",
    "## Деревья решений и ансамблевые методы\n",
    "\n",
    "В этой лабораторной работе вам предстоит реализовать с нуля алгоритм дерева решений, а затем применить и сравнить различные ансамблевые методы, доступные в библиотеке `scikit-learn`.\n",
    "\n",
    "### Цель\n",
    "\n",
    "- Изучить теоретические основы и получить практические навыки реализации деревьев решений.\n",
    "- Освоить применение ансамблевых методов (бэггинг, бустинг) для решения задачи классификации.\n",
    "- Научиться сравнивать и анализировать производительность различных моделей машинного обучения.\n",
    "\n",
    "### Оценивание и баллы\n",
    "\n",
    "За это задание в общей сложности можно получить **до 10 баллов**. Баллы распределяются по задачам, как описано в ячейках ниже. Чтобы получить максимальный балл, необходимо успешно выполнить все обязательные задачи."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YhdhwvrCw9c5"
   },
   "source": [
    "***\n",
    "### Задачи\n",
    "\n",
    "#### 1. Определить номер варианта\n",
    "Перейдите по ссылке из личного кабинета на Google Таблицу со списком студентов. Найдите свое ФИО в списке и запомните соответствующий порядковый номер (поле № п/п) в первом столбце. Заполните его в ячейке ниже и выполните ячейку. Если вы не можете найти себя в списке, обратитесь к своему преподавателю."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l8k1q37Ww9c5"
   },
   "outputs": [],
   "source": [
    "# TODO: Впишите свой номер по списку (STUDENT_ID)\n",
    "STUDENT_ID = 9 # Для 10-го варианта номер 9 (индексация с 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cI8EZ9E6w9c5"
   },
   "source": [
    "Теперь выполните следующую ячейку. Она определит ваш вариант задания и выведет его."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JzKTd-a1w9c6",
    "outputId": "3d665890-2340-49f3-9e13-62cb665ae78c",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "if STUDENT_ID is None:\n",
    "    print(\"ОШИБКА! Не указан порядковый номер студента в списке группы.\")\n",
    "else:\n",
    "    variants = pd.DataFrame([\n",
    "        {\"Dataset\": \"Прогнозирование оттока клиентов банка\", \"Dataset URL\": \"shubhammeshram579/bank-customer-churn-prediction\", \"Tree Algo\": \"ID3\", \"Bagging Algo\": \"RandomForestClassifier\", \"Boosting Algo\": \"AdaBoostClassifier\"},\n",
    "        {\"Dataset\": \"Прогнозирование инсульта\", \"Dataset URL\": \"fedesoriano/stroke-prediction-dataset\", \"Tree Algo\": \"C4.5\", \"Bagging Algo\": \"BaggingClassifier\", \"Boosting Algo\": \"GradientBoostingClassifier\"},\n",
    "        {\"Dataset\": \"Качество красного вина\", \"Dataset URL\": \"uciml/red-wine-quality-cortez-et-al-2009\", \"Tree Algo\": \"ID3\", \"Bagging Algo\": \"RandomForestClassifier\", \"Boosting Algo\": \"XGBoost\"},\n",
    "        {\"Dataset\": \"Прогнозирование сердечной недостаточности\", \"Dataset URL\": \"fedesoriano/heart-failure-prediction\", \"Tree Algo\": \"C4.5\", \"Bagging Algo\": \"ExtraTreesClassifier\", \"Boosting Algo\": \"CatBoost\"},\n",
    "        {\"Dataset\": \"Набор данных о курении\", \"Dataset URL\": \"kukuroo3/body-signal-of-smoking\", \"Tree Algo\": \"ID3\", \"Bagging Algo\": \"BaggingClassifier\", \"Boosting Algo\": \"AdaBoostClassifier\"},\n",
    "        {\"Dataset\": \"Удержание клиентов телеком-оператора\", \"Dataset URL\": \"blastchar/telco-customer-churn\", \"Tree Algo\": \"C4.5\", \"Bagging Algo\": \"RandomForestClassifier\", \"Boosting Algo\": \"GradientBoostingClassifier\"},\n",
    "        {\"Dataset\": \"Покупка в социальных сетях\", \"Dataset URL\": \"rakeshpanigrahi/social-network-ads\", \"Tree Algo\": \"ID3\", \"Bagging Algo\": \"ExtraTreesClassifier\", \"Boosting Algo\": \"XGBoost\"},\n",
    "        {\"Dataset\": \"Оценка риска по кредиту\", \"Dataset URL\": \"uciml/german-credit\", \"Tree Algo\": \"C4.5\", \"Bagging Algo\": \"BaggingClassifier\", \"Boosting Algo\": \"CatBoost\"},\n",
    "        {\"Dataset\": \"Прогнозирование диабета\", \"Dataset URL\": \"uciml/pima-indians-diabetes-database\", \"Tree Algo\": \"ID3\", \"Bagging Algo\": \"RandomForestClassifier\", \"Boosting Algo\": \"AdaBoostClassifier\"},\n",
    "        {\"Dataset\": \"Обнаружение мошенничества с онлайн-платежами\", \"Dataset URL\": \"rupakroy/online-payments-fraud-detection-dataset\", \"Tree Algo\": \"C4.5\", \"Bagging Algo\": \"BaggingClassifier\", \"Boosting Algo\": \"GradientBoostingClassifier\"},\n",
    "    ])\n",
    "    variant = variants.iloc[STUDENT_ID % len(variants)]\n",
    "    print(f\"Ваш вариант: {STUDENT_ID % len(variants) + 1}\")\n",
    "    print(f\"\\nДатасет: {variant['Dataset']}\")\n",
    "    print(f\"URL для Kaggle API: {variant['Dataset URL']}\")\n",
    "    print(f\"\\nЧасть 1. Алгоритм дерева для реализации: {variant['Tree Algo']}\")\n",
    "    print(f\"Часть 2. Алгоритмы для сравнения:\")\n",
    "    print(f\"  - Бэггинг: {variant['Bagging Algo']}\")\n",
    "    print(f\"  - Бустинг: {variant['Boosting Algo']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6wtVjnqUw9c6"
   },
   "source": [
    "#### 2. Загрузка и подготовка данных\n",
    "\n",
    "Для загрузки датасета из Kaggle рекомендуется использовать Kaggle API. Это избавит вас от необходимости скачивать файлы вручную.\n",
    "\n",
    "**Инструкция по настройке Kaggle API в Google Colab:**\n",
    "1.  Зайдите в свой профиль на Kaggle, перейдите в раздел `Account`.\n",
    "2.  Нажмите на кнопку `Create New API Token`. На ваш компьютер скачается файл `kaggle.json`.\n",
    "3.  Выполните ячейку с кодом ниже. Она предложит вам загрузить файл. Выберите скачанный `kaggle.json`.\n",
    "4.  После этого вы сможете скачивать датасеты с помощью команд `!kaggle datasets download ...`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "# Загружаем файл kaggle.json\n",
    "if not os.path.exists('/root/.kaggle/kaggle.json'):\n",
    "    # Эта ячейка запросит загрузку файла. Загрузите сюда ваш kaggle.json\n",
    "    uploaded = files.upload()\n",
    "    for fn in uploaded.keys():\n",
    "        print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
    "            name=fn, length=len(uploaded[fn])))\n",
    "    # Создаем папку и перемещаем в нее файл\n",
    "    !mkdir -p ~/.kaggle\n",
    "    !mv kaggle.json ~/.kaggle/\n",
    "    !chmod 600 ~/.kaggle/kaggle.json\n",
    "    print(\"Kagle API token successfully set up.\")\n",
    "else:\n",
    "    print(\"Kaggle API token already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nrsth8kTw9c6"
   },
   "source": [
    "Теперь, используя URL для Kaggle API из вашего варианта, скачайте и распакуйте датасет. Загрузите данные в DataFrame библиотеки Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R_QmxpB5w9c6"
   },
   "outputs": [],
   "source": [
    "KAGGLE_DATASET_URL = variant['Dataset URL']\n",
    "archive_name = KAGGLE_DATASET_URL.split('/')[1] + '.zip'\n",
    "\n",
    "!kaggle datasets download -d {KAGGLE_DATASET_URL}\n",
    "\n",
    "import zipfile\n",
    "with zipfile.ZipFile(archive_name, 'r') as zip_ref:\n",
    "    zip_ref.extractall('./data')\n",
    "\n",
    "# Загружаем данные в DataFrame\n",
    "dataset = pd.read_csv('data/PS_20174392719_1491204439457_log.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l0lLa-xiR101"
   },
   "source": [
    "#### 3. Анализ и предварительная обработка данных (1 балл)\n",
    "\n",
    "Прежде чем строить модели, необходимо изучить данные. Проведите базовый анализ:\n",
    "\n",
    "1.  **Изучите общую информацию о датасете:** размер, типы признаков, наличие пропусков.\n",
    "2.  **Проанализируйте целевую переменную:** посмотрите на распределение классов. Является ли выборка сбалансированной?\n",
    "3.  **Обработайте пропуски:** выберите стратегию для заполнения или удаления пропущенных значений.\n",
    "4.  **Обработайте категориальные признаки:** используйте one-hot encoding, label encoding или другие методы для преобразования текстовых признаков в числовые.\n",
    "5.  **Разделите данные:** разбейте датасет на обучающую и тестовую выборки (`train_test_split` из `sklearn.model_selection`).\n",
    "6.  **Масштабируйте признаки:** при необходимости примените масштабирование (например, `StandardScaler` или `MinMaxScaler` из `sklearn.preprocessing`).\n",
    "\n",
    "В ячейках ниже выполните необходимые шаги и напишите краткие выводы по каждому пункту."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ujitZUYeR101"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Общая информация\n",
    "print(\"Общая информация о датасете:\")\n",
    "dataset.info()\n",
    "print(\"\\nПервые 5 строк:\")\n",
    "print(dataset.head())\n",
    "\n",
    "# 2. Анализ целевой переменной\n",
    "print(\"\\nРаспределение классов целевой переменной 'isFraud':\")\n",
    "print(dataset['isFraud'].value_counts())\n",
    "sns.countplot(x='isFraud', data=dataset)\n",
    "plt.title('Распределение классов')\n",
    "plt.show()\n",
    "# Вывод: классы сильно несбалансированы. Это нужно будет учесть.\n",
    "\n",
    "# 3. Обработка пропусков\n",
    "print(f\"\\nКоличество пропущенных значений:\\n{dataset.isnull().sum()}\")\n",
    "# Вывод: пропущенных значений нет.\n",
    "\n",
    "# 4. Обработка категориальных признаков\n",
    "# Удалим ненужные признаки\n",
    "dataset = dataset.drop(['nameOrig', 'nameDest', 'isFlaggedFraud'], axis=1)\n",
    "\n",
    "# Признак 'type' является категориальным\n",
    "categorical_features = ['type']\n",
    "numeric_features = ['step', 'amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']\n",
    "\n",
    "# Создаем трансформер для one-hot encoding\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(), categorical_features)])\n",
    "\n",
    "# 5. Разделение данных\n",
    "X = dataset.drop('isFraud', axis=1)\n",
    "y = dataset['isFraud']\n",
    "\n",
    "# Из-за большого размера датасета возьмем только часть данных для ускорения работы\n",
    "# Stratify используется для сохранения пропорций классов\n",
    "_, X_sample, _, y_sample = train_test_split(X, y, test_size=0.1, random_state=42, stratify=y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sample, y_sample, test_size=0.3, random_state=42, stratify=y_sample)\n",
    "\n",
    "print(f\"\\nРазмер обучающей выборки: {X_train.shape}\")\n",
    "print(f\"Размер тестовой выборки: {X_test.shape}\")\n",
    "\n",
    "# 6. Масштабирование и кодирование признаков\n",
    "X_train_prepared = preprocessor.fit_transform(X_train)\n",
    "X_test_prepared = preprocessor.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FGsJ_-hMXjSs"
   },
   "source": [
    "**Выводы по анализу и предобработке:**\n",
    "\n",
    "- **Общая информация:** Датасет содержит более 6 миллионов записей и 11 признаков. Пропущенных значений нет. Признаки `nameOrig`, `nameDest` и `isFlaggedFraud` были удалены как несущественные для модели.\n",
    "- **Целевая переменная:** Классы сильно несбалансированы. Мошеннических транзакций (`isFraud` = 1) всего около 0.12%. Это означает, что метрика Accuracy может быть обманчивой, и важно будет смотреть на Precision, Recall и ROC-AUC.\n",
    "- **Предобработка:** Категориальный признак `type` был преобразован с помощью One-Hot Encoding. Все числовые признаки были отмасштабированы с помощью StandardScaler для корректной работы моделей.\n",
    "- **Выборка:** Из-за огромного размера исходного датасета для обучения и тестирования была взята случайная стратифицированная выборка в 10% от общего объема, чтобы ускорить вычисления, сохранив при этом исходное соотношение классов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xfw6nVzoR104"
   },
   "source": [
    "---\n",
    "### Часть 1: Реализация дерева решений (4 балла)\n",
    "\n",
    "В этой части вам предстоит реализовать алгоритм построения дерева решений для задачи классификации с нуля, используя только `NumPy`. \n",
    "\n",
    "**Критерий разделения:**\n",
    "-   Если ваш алгоритм - **ID3**, используйте **Information Gain**.\n",
    "-   Если ваш алгоритм - **C4.5**, используйте **Gain Ratio**.\n",
    "\n",
    "Вам нужно будет реализовать две основные сущности:\n",
    "1.  `Node` — узел дерева.\n",
    "2.  `DecisionTreeClassifier` — сам классификатор."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KI2IIJdHR104"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Node:\n",
    "    \"\"\"Класс, представляющий узел в дереве решений.\"\"\"\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, *, value=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            feature (int): Индекс признака для разделения.\n",
    "            threshold (float): Пороговое значение для разделения.\n",
    "            left (Node): Левый дочерний узел (для значений <= threshold).\n",
    "            right (Node): Правый дочерний узел (для значений > threshold).\n",
    "            value (int): Значение класса (если узел является листом).\n",
    "        \"\"\"\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "\n",
    "    def is_leaf_node(self):\n",
    "        \"\"\"Проверяет, является ли узел листовым.\"\"\"\n",
    "        return self.value is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XOhAxyK1R107"
   },
   "outputs": [],
   "source": [
    "# Внимание: нельзя использовать готовые реализации деревьев решений!\n",
    "\n",
    "class DecisionTreeClassifier:\n",
    "    \"\"\"Классификатор на основе дерева решений.\"\"\"\n",
    "    def __init__(self, min_samples_split=2, max_depth=100, n_feats=None, criterion='id3'):\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.n_feats = n_feats\n",
    "        self.root = None\n",
    "        self.criterion = criterion # 'id3' for Information Gain, 'c4.5' for Gain Ratio\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Обучает дерево решений.\"\"\"\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        self.n_feats = X.shape[1] if not self.n_feats else min(self.n_feats, X.shape[1])\n",
    "        self.root = self._grow_tree(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Делает предсказания для новых данных.\"\"\"\n",
    "        X = np.array(X)\n",
    "        return np.array([self._traverse_tree(x, self.root) for x in X])\n",
    "\n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        \"\"\"Рекурсивно строит дерево.\"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        n_labels = len(np.unique(y))\n",
    "\n",
    "        # Критерии остановки\n",
    "        if (depth >= self.max_depth or n_labels == 1 or n_samples < self.min_samples_split):\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return Node(value=leaf_value)\n",
    "\n",
    "        feat_idxs = np.random.choice(n_features, self.n_feats, replace=False)\n",
    "\n",
    "        # Находим лучшее разделение\n",
    "        best_feat, best_thresh = self._best_criteria(X, y, feat_idxs)\n",
    "\n",
    "        if best_feat is None:\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return Node(value=leaf_value)\n",
    "        \n",
    "        # Разделяем данные\n",
    "        left_idxs, right_idxs = self._split(X[:, best_feat], best_thresh)\n",
    "        left = self._grow_tree(X[left_idxs, :], y[left_idxs], depth + 1)\n",
    "        right = self._grow_tree(X[right_idxs, :], y[right_idxs], depth + 1)\n",
    "        return Node(best_feat, best_thresh, left, right)\n",
    "\n",
    "    def _best_criteria(self, X, y, feat_idxs):\n",
    "        \"\"\"Выбирает лучший признак и порог для разделения.\"\"\"\n",
    "        best_gain = -1\n",
    "        split_idx, split_thresh = None, None\n",
    "        for feat_idx in feat_idxs:\n",
    "            X_column = X[:, feat_idx]\n",
    "            thresholds = np.unique(X_column)\n",
    "            for threshold in thresholds:\n",
    "                if self.criterion == 'id3':\n",
    "                    gain = self._information_gain(y, X_column, threshold)\n",
    "                elif self.criterion == 'c4.5':\n",
    "                    gain = self._gain_ratio(y, X_column, threshold)\n",
    "                else:\n",
    "                    raise ValueError(\"Критерий должен быть 'id3' или 'c4.5'\")\n",
    "\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    split_idx = feat_idx\n",
    "                    split_thresh = threshold\n",
    "        return split_idx, split_thresh\n",
    "\n",
    "    def _information_gain(self, y, X_column, split_thresh):\n",
    "        \"\"\"Вычисляет Information Gain.\"\"\"\n",
    "        parent_entropy = self._entropy(y)\n",
    "        left_idxs, right_idxs = self._split(X_column, split_thresh)\n",
    "        if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
    "            return 0\n",
    "        n = len(y)\n",
    "        n_l, n_r = len(left_idxs), len(right_idxs)\n",
    "        e_l, e_r = self._entropy(y[left_idxs]), self._entropy(y[right_idxs])\n",
    "        child_entropy = (n_l / n) * e_l + (n_r / n) * e_r\n",
    "        ig = parent_entropy - child_entropy\n",
    "        return ig\n",
    "\n",
    "    def _gain_ratio(self, y, X_column, split_thresh):\n",
    "        \"\"\"Вычисляет Gain Ratio.\"\"\"\n",
    "        info_gain = self._information_gain(y, X_column, split_thresh)\n",
    "        split_info_val = self._split_info(y, X_column, split_thresh)\n",
    "        # Избегаем деления на ноль\n",
    "        if split_info_val == 0:\n",
    "            return 0\n",
    "        return info_gain / split_info_val\n",
    "\n",
    "    def _entropy(self, y):\n",
    "        \"\"\"Вычисляет энтропию.\"\"\"\n",
    "        hist = np.bincount(y)\n",
    "        ps = hist / len(y)\n",
    "        return -np.sum([p * np.log2(p) for p in ps if p > 0])\n",
    "\n",
    "    def _split_info(self, y, X_column, split_thresh):\n",
    "        \"\"\"Вычисляет Split Information для Gain Ratio.\"\"\"\n",
    "        left_idxs, right_idxs = self._split(X_column, split_thresh)\n",
    "        n = len(y)\n",
    "        n_l, n_r = len(left_idxs), len(right_idxs)\n",
    "        if n_l == 0 or n_r == 0:\n",
    "             return 0 # Деления нет, информация о разделении равна 0\n",
    "        p_l = n_l / n\n",
    "        p_r = n_r / n\n",
    "        return - (p_l * np.log2(p_l) + p_r * np.log2(p_r))\n",
    "\n",
    "    def _split(self, X_column, split_thresh):\n",
    "        \"\"\"Разделяет данные по порогу.\"\"\"\n",
    "        left_idxs = np.argwhere(X_column <= split_thresh).flatten()\n",
    "        right_idxs = np.argwhere(X_column > split_thresh).flatten()\n",
    "        return left_idxs, right_idxs\n",
    "\n",
    "    def _traverse_tree(self, x, node):\n",
    "        \"\"\"Проходит по дереву для предсказания одного сэмпла.\"\"\"\n",
    "        if node.is_leaf_node():\n",
    "            return node.value\n",
    "\n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self._traverse_tree(x, node.left)\n",
    "        return self._traverse_tree(x, node.right)\n",
    "\n",
    "    def _most_common_label(self, y):\n",
    "        \"\"\"Находит самый частый класс в наборе данных.\"\"\"\n",
    "        if len(y) == 0:\n",
    "            return 0 # Возвращаем дефолтное значение, если подвыборка пуста\n",
    "        unique, counts = np.unique(y, return_counts=True)\n",
    "        return unique[np.argmax(counts)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9mbKbih2R109"
   },
   "source": [
    "Теперь обучите свой классификатор на обучающей выборке и оцените его качество на тестовой. Рассчитайте метрику **Accuracy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sBdljd94R109"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Создаем и обучаем экземпляр классификатора\n",
    "# Устанавливаем criterion='c4.5' согласно варианту\n",
    "custom_tree = DecisionTreeClassifier(max_depth=10, criterion='c4.5')\n",
    "custom_tree.fit(X_train_prepared.toarray(), y_train.to_numpy())\n",
    "\n",
    "# Делаем предсказание\n",
    "y_pred_custom = custom_tree.predict(X_test_prepared.toarray())\n",
    "\n",
    "# Оцениваем точность\n",
    "custom_tree_accuracy = accuracy_score(y_test, y_pred_custom)\n",
    "\n",
    "print(f\"Точность (Accuracy) вашего дерева решений: {custom_tree_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_OSBr-iNR10_"
   },
   "source": [
    "---\n",
    "### Часть 2: Применение и сравнение ансамблевых методов (3 балла)\n",
    "\n",
    "Теперь воспользуемся готовыми реализациями из `scikit-learn` для применения ансамблевых методов.\n",
    "\n",
    "1.  Импортируйте и создайте экземпляры классификаторов для **бэггинга** и **бустинга** согласно вашему варианту.\n",
    "2.  Обучите обе модели на **обучающей** выборке.\n",
    "3.  Сделайте предсказания на **тестовой** выборке.\n",
    "4.  Рассчитайте метрики **Accuracy, Precision, Recall, F1-score** и **ROC-AUC** для каждой модели.\n",
    "5.  Постройте **ROC-кривые** для обеих моделей на одном графике для наглядного сравнения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X4t5jNtbR11B"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Создание моделей\n",
    "bagging_model = BaggingClassifier(random_state=42)\n",
    "boosting_model = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# 2. Обучение моделей\n",
    "print(\"Обучение BaggingClassifier...\")\n",
    "bagging_model.fit(X_train_prepared, y_train)\n",
    "print(\"Обучение GradientBoostingClassifier...\")\n",
    "boosting_model.fit(X_train_prepared, y_train)\n",
    "print(\"Обучение завершено.\")\n",
    "\n",
    "# 3. Предсказания\n",
    "y_pred_bagging = bagging_model.predict(X_test_prepared)\n",
    "y_pred_boosting = boosting_model.predict(X_test_prepared)\n",
    "y_proba_bagging = bagging_model.predict_proba(X_test_prepared)[:, 1]\n",
    "y_proba_boosting = boosting_model.predict_proba(X_test_prepared)[:, 1]\n",
    "\n",
    "# 4. Расчет метрик\n",
    "print(\"\\n--- Метрики для BaggingClassifier ---\")\n",
    "print(classification_report(y_test, y_pred_bagging))\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_test, y_proba_bagging):.4f}\")\n",
    "\n",
    "print(\"\\n--- Метрики для GradientBoostingClassifier ---\")\n",
    "print(classification_report(y_test, y_pred_boosting))\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_test, y_proba_boosting):.4f}\")\n",
    "\n",
    "# 5. Построение ROC-кривых\n",
    "fpr_bag, tpr_bag, _ = roc_curve(y_test, y_proba_bagging)\n",
    "fpr_boost, tpr_boost, _ = roc_curve(y_test, y_proba_boosting)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(fpr_bag, tpr_bag, label=f'Bagging (AUC = {roc_auc_score(y_test, y_proba_bagging):.4f})')\n",
    "plt.plot(fpr_boost, tpr_boost, label=f'Gradient Boosting (AUC = {roc_auc_score(y_test, y_proba_boosting):.4f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC-кривые')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m5el9T4eR11D"
   },
   "source": [
    "**Сравнение моделей:**\n",
    "\n",
    "- **Точность (Accuracy):** Обе модели показывают очень высокую точность (близкую к 99.9%), но это обманчивый показатель из-за сильного дисбаланса классов. Модель может просто предсказывать для всех транзакций класс \"не мошенничество\" и быть почти всегда правой.\n",
    "\n",
    "- **Precision и Recall (для класса 1 - 'fraud'):** Здесь видна реальная разница. \n",
    "  - `BaggingClassifier` показывает неплохой `recall` (полноту), то есть он способен найти значительную часть мошеннических транзакций. Однако его `precision` (точность) ниже, что означает, что среди предсказанных им мошеннических операций много ложных срабатываний.\n",
    "  - `GradientBoostingClassifier`, наоборот, демонстрирует более высокую `precision`, то есть если он помечает транзакцию как мошенническую, то с большей вероятностью это так и есть. Но его `recall` ниже, значит, он пропускает большее количество реальных мошенничеств.\n",
    "\n",
    "- **ROC-AUC:** Площадь под ROC-кривой у обеих моделей очень высока (около 0.98-0.99), что говорит о высокой общей предсказательной способности. `GradientBoostingClassifier` показывает немного лучший результат, что видно и на графике: его кривая проходит чуть выше кривой бэггинга.\n",
    "\n",
    "**Вывод:** В задаче обнаружения мошенничества обычно важнее максимизировать `Recall` (не пропустить мошенников), даже ценой снижения `Precision` (допуская ложные тревоги, которые можно проверить вручную). С этой точки зрения, базовая модель `BaggingClassifier` выглядит предпочтительнее. Однако, `GradientBoosting` показывает лучший общий баланс и более высокую площадь под кривой, что делает его более мощным алгоритмом в целом. Его можно было бы донастроить (например, изменить порог классификации), чтобы увеличить полноту."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vmaBPxD_R11G"
   },
   "source": [
    "---\n",
    "#### 8. Опишите полученные результаты (1 балл)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zI7TNvZsR11H"
   },
   "source": [
    "Напишите краткие выводы объемом в один абзац, ориентированные на нетехническую аудиторию (например, на вашего менеджера или начальника). Сосредоточьтесь на следующих вопросах:\n",
    "\n",
    "- Какое из решений (ваше дерево, бэггинг, бустинг) вы бы порекомендовали для решения бизнес-задачи?\n",
    "- Каковы основные результаты и что они означают на практике (например, \"наша модель с точностью 95% определяет потенциально мошеннические транзакции\")?\n",
    "- Какие дальнейшие шаги по улучшению модели вы бы предложили?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CtMU8QhXPFK8"
   },
   "source": [
    "**Выводы для нетехнической аудитории:**\n",
    "\n",
    "Мы разработали и сравнили несколько моделей для автоматического выявления мошеннических онлайн-платежей. Для этой бизнес-задачи я бы порекомендовал использовать модель на основе **градиентного бустинга**. На практике это означает, что наша система способна с высокой долей вероятности (точность около 88%) правильно идентифицировать мошенническую операцию, при этом обнаруживая примерно 60% от всех случаев мошенничества. Хотя модель бэггинга находит больше мошенников (полнота 75%), она чаще ошибается, помечая легитимные операции как подозрительные. В качестве дальнейших шагов я предлагаю провести более глубокую настройку модели бустинга и использовать специальные техники для работы с несбалансированными данными, что позволит нам увеличить количество выявляемых мошеннических транзакций, не сильно увеличивая число ложных тревог."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I1vTd45lR11J"
   },
   "source": [
    "---\n",
    "### Нужна помощь?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ionUS2eLR11K"
   },
   "source": [
    "Если у вас возникли трудности при выполнении задания, попробуйте следующие решения:\n",
    "\n",
    "- Посмотрите слайды к лекциям по деревьям решений и ансамблевым методам. Слайды можно найти в личном кабинете или в ТГ-канале курса.\n",
    "- Задайте вопрос преподавателю в ТГ-канале курса.\n",
    "- Задайте вопрос преподавателю лично в университете."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
